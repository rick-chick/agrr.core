# xfailed ãƒ†ã‚¹ãƒˆ - ä»•æ§˜ææ¡ˆæ›¸

å„xfailedãƒ†ã‚¹ãƒˆã«ã¤ã„ã¦ã€å®Ÿè£…ã™ã¹ãä»•æ§˜ã®å…·ä½“æ¡ˆã‚’æç¤ºã—ã¾ã™ã€‚

---

## 1. test_missing_required_columns

### ç¾çŠ¶
- HTMLãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å¿…é ˆã‚«ãƒ©ãƒ ãŒæ¬ è½ã—ã¦ã„ã‚‹å ´åˆã®å‡¦ç†ãŒæœªå®šç¾©
- ç¾åœ¨ã¯è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãŒä¸ååˆ†

### ä»•æ§˜æ¡ˆA: Strict ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰

**æ–¹é‡**: å¿…é ˆãƒ‡ãƒ¼ã‚¿ãŒæ¬ è½ã—ã¦ã„ã‚‹è¡Œã¯ç„¡åŠ¹ã¨ã¿ãªã™

```python
def _parse_row(self, row: TableRow, year: int, month: int) -> Optional[WeatherData]:
    """
    Parse a single row from JMA table.
    
    å¿…é ˆã‚«ãƒ©ãƒ :
    - cells[0]: æ—¥ï¼ˆå¿…é ˆï¼‰
    - cells[3]: é™æ°´é‡ï¼ˆå¿…é ˆï¼‰
    - cells[6]: å¹³å‡æ°—æ¸©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - cells[7]: æœ€é«˜æ°—æ¸©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - cells[8]: æœ€ä½æ°—æ¸©ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - cells[16]: æ—¥ç…§æ™‚é–“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        WeatherData or None if row is invalid
    """
    # æœ€ä½é™å¿…è¦ãªã‚«ãƒ©ãƒ æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    MIN_REQUIRED_CELLS = 9  # cells[0-8]ã¾ã§å¿…é ˆ
    
    if len(row.cells) < MIN_REQUIRED_CELLS:
        self.logger.warning(
            f"Row has insufficient columns: {len(row.cells)} < {MIN_REQUIRED_CELLS}. "
            f"Skipping row."
        )
        return None
    
    # æ—¥ä»˜ã®å¿…é ˆãƒã‚§ãƒƒã‚¯
    day_str = row.cells[0].strip()
    if not day_str or not day_str.isdigit():
        self.logger.warning(f"Invalid day value: '{day_str}'. Skipping row.")
        return None
    
    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    day = int(day_str)
    precipitation = self._safe_float(row.cells[3])
    temp_mean = self._safe_float(row.cells[6])
    temp_max = self._safe_float(row.cells[7])
    temp_min = self._safe_float(row.cells[8])
    
    # æ°—æ¸©ãŒå…¨ã¦Nullã®å ´åˆã¯è­¦å‘Š
    if temp_max is None and temp_min is None and temp_mean is None:
        self.logger.warning(
            f"All temperature values are None for {year}-{month:02d}-{day:02d}. "
            f"Data quality issue."
        )
        # ã“ã®ã‚±ãƒ¼ã‚¹ã¯è¨±å®¹ã™ã‚‹ã‹ï¼Ÿ
        # Option A: è¨±å®¹ï¼ˆé™æ°´é‡ãƒ‡ãƒ¼ã‚¿ã¯æœ‰åŠ¹ï¼‰
        # Option B: æ‹’å¦ï¼ˆæ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã¯é‡è¦ï¼‰
    
    # ... ä»¥ä¸‹ç¶šã
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ‡ãƒ¼ã‚¿å“è³ªãŒä¿è¨¼ã•ã‚Œã‚‹
- ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãŒæ—©ã„
- ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ‡ãƒ¼ã‚¿å–å¾—é‡ãŒæ¸›ã‚‹å¯èƒ½æ€§

### ä»•æ§˜æ¡ˆB: Lenient ãƒ¢ãƒ¼ãƒ‰

**æ–¹é‡**: å¯èƒ½ãªé™ã‚Šãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€æ¬ æã¯ None ã§åŸ‹ã‚ã‚‹

```python
def _parse_row(self, row: TableRow, year: int, month: int) -> Optional[WeatherData]:
    # æœ€ä½é™ã€æ—¥ä»˜ã¨1ã¤ä»¥ä¸Šã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°OK
    if len(row.cells) < 4:  # æ—¥ä»˜ + ä½•ã‹ã—ã‚‰ã®ãƒ‡ãƒ¼ã‚¿
        return None
    
    # å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’Optionalã¨ã—ã¦æ‰±ã†
    day = self._safe_int(row.cells[0])
    if day is None:
        return None
    
    precipitation = self._safe_float(row.cells[3] if len(row.cells) > 3 else None)
    temp_mean = self._safe_float(row.cells[6] if len(row.cells) > 6 else None)
    temp_max = self._safe_float(row.cells[7] if len(row.cells) > 7 else None)
    temp_min = self._safe_float(row.cells[8] if len(row.cells) > 8 else None)
    
    return WeatherData(...)  # å…¨ã¦Noneã‚‚è¨±å®¹
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ‡ãƒ¼ã‚¿å–å¾—é‡ãŒæœ€å¤§åŒ–
- éƒ¨åˆ†çš„ãªãƒ‡ãƒ¼ã‚¿ã‚‚æ´»ç”¨å¯èƒ½

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- ä¸‹æµã§ã® None ãƒã‚§ãƒƒã‚¯ãŒå¿…è¦
- ãƒ‡ãƒ¼ã‚¿å“è³ªãŒä¸å®‰å®š

### æ¨å¥¨: **ä»•æ§˜æ¡ˆAï¼ˆStrict ãƒ¢ãƒ¼ãƒ‰ï¼‰- æ°—æ¸©ã®ã¿å¿…é ˆ**
- è¾²æ¥­äºˆæ¸¬ã§ã¯æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ãŒæœ€é‡è¦ï¼ˆGDDè¨ˆç®—ã«å¿…é ˆï¼‰
- **é™æ°´é‡ã€æ—¥ç…§ã€æ¹¿åº¦ãªã©ã¯æ¬ æOK**ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³æ‰±ã„ï¼‰
- æ°—æ¸©ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è¨¼ã—ã¤ã¤ã€ãƒ‡ãƒ¼ã‚¿å–å¾—é‡ã‚’æœ€å¤§åŒ–
- æŸ”è»Ÿæ€§ã¨å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ã‚’å®Ÿç¾

---

## 2. test_duplicate_dates_in_csv

### ç¾çŠ¶
- åŒã˜æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°å›å‡ºç¾ã—ãŸå ´åˆã®å‡¦ç†ãŒæœªå®šç¾©
- ç¾åœ¨ã¯ãã®ã¾ã¾å‡¦ç†ï¼ˆé‡è¤‡ãŒæ··å…¥ã™ã‚‹å¯èƒ½æ€§ï¼‰

### ä»•æ§˜æ¡ˆA: First Winsï¼ˆæœ€åˆã®å€¤ã‚’æ¡ç”¨ï¼‰

```python
def _parse_jma_table(self, table: HtmlTable, ...) -> List[WeatherData]:
    weather_data_dict = {}  # Dict[datetime, WeatherData]
    
    for row in table.rows:
        record_date = datetime(year, month, day)
        
        # æ—¢ã«å­˜åœ¨ã™ã‚‹æ—¥ä»˜ã¯ã‚¹ã‚­ãƒƒãƒ—
        if record_date in weather_data_dict:
            self.logger.warning(
                f"Duplicate date detected: {record_date.date()}. "
                f"Using first occurrence."
            )
            continue
        
        weather_data = WeatherData(...)
        weather_data_dict[record_date] = weather_data
    
    return list(weather_data_dict.values())
```

**ãƒ¡ãƒªãƒƒãƒˆ**: ã‚·ãƒ³ãƒ—ãƒ«ã€äºˆæ¸¬å¯èƒ½
**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§

### ä»•æ§˜æ¡ˆB: Last Winsï¼ˆæœ€å¾Œã®å€¤ã‚’æ¡ç”¨ï¼‰

```python
# æœ€å¾Œã®ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ã
weather_data_dict[record_date] = weather_data  # å¸¸ã«ä¸Šæ›¸ã
```

**ãƒ¡ãƒªãƒƒãƒˆ**: ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ã‚’åæ˜ å¯èƒ½
**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ä¾å­˜

### ä»•æ§˜æ¡ˆC: Averageï¼ˆå¹³å‡ã‚’å–ã‚‹ï¼‰

```python
if record_date in weather_data_dict:
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å¹³å‡
    existing = weather_data_dict[record_date]
    weather_data = self._merge_weather_data(existing, weather_data)

def _merge_weather_data(self, data1: WeatherData, data2: WeatherData) -> WeatherData:
    """Merge two weather data by averaging."""
    return WeatherData(
        time=data1.time,
        temperature_2m_max=(
            (data1.temperature_2m_max + data2.temperature_2m_max) / 2
            if data1.temperature_2m_max and data2.temperature_2m_max
            else data1.temperature_2m_max or data2.temperature_2m_max
        ),
        # ... ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚åŒæ§˜
    )
```

**ãƒ¡ãƒªãƒƒãƒˆ**: çµ±è¨ˆçš„ã«å¦¥å½“
**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: è¤‡é›‘ã€è¨ˆç®—ã‚³ã‚¹ãƒˆ

### ä»•æ§˜æ¡ˆD: Strict Errorï¼ˆã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ã‚‹ï¼‰

```python
if record_date in weather_data_dict:
    raise WeatherAPIError(
        f"Duplicate date detected: {record_date.date()}. "
        f"Data quality issue in source."
    )
```

**ãƒ¡ãƒªãƒƒãƒˆ**: ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã‚’æ—©æœŸç™ºè¦‹
**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: ãƒ‡ãƒ¼ã‚¿å–å¾—ãŒå¤±æ•—ã—ã‚„ã™ã„

### æ¨å¥¨: **ä»•æ§˜æ¡ˆAï¼ˆFirst Winsï¼‰+ ãƒ­ã‚°è­¦å‘Š**
- ã‚·ãƒ³ãƒ—ãƒ«ã§äºˆæ¸¬å¯èƒ½
- é‡è¤‡ã‚’è­¦å‘Šãƒ­ã‚°ã§è¨˜éŒ²
- å®Ÿç”¨ä¸Šååˆ†

---

## 3. test_session_cleanup_on_error

### ç¾çŠ¶
- `CsvDownloader`ã§ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œãªã„
- ãƒªã‚½ãƒ¼ã‚¹ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§

### ä»•æ§˜æ¡ˆA: Context Manager ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ¨å¥¨ï¼‰

```python
# src/agrr_core/framework/repositories/csv_downloader.py

class CsvDownloader(CsvServiceInterface):
    """CSV downloader with proper resource management."""
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None  # é…å»¶åˆæœŸåŒ–
    
    def __enter__(self):
        """Context manager entry."""
        self.session = requests.Session()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        if self.session:
            self.session.close()
            self.session = None
        return False
    
    async def download_csv(self, url: str, encoding: str = 'utf-8') -> pd.DataFrame:
        """Download CSV with automatic cleanup."""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯ä½œæˆ
        if self.session is None:
            self.session = requests.Session()
        
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            csv_text = response.content.decode(encoding)
            df = pd.read_csv(StringIO(csv_text))
            
            return df
            
        except requests.RequestException as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.session:
                self.session.close()
                self.session = None
            raise CsvDownloadError(f"Failed to download CSV: {e}")
    
    def __del__(self):
        """Destructor with final cleanup."""
        if self.session:
            self.session.close()
```

**ä½¿ç”¨ä¾‹**:
```python
# ä½¿ç”¨å´ã§Context Managerã‚’ä½¿ç”¨
with CsvDownloader(timeout=30) as downloader:
    df = await downloader.download_csv(url)
# è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- Pythonic ãªå®Ÿè£…
- è‡ªå‹•ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
- ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### ä»•æ§˜æ¡ˆB: Finally Block

```python
async def download_csv(self, url: str, encoding: str = 'utf-8') -> pd.DataFrame:
    try:
        response = self.session.get(url, timeout=self.timeout)
        # ... å‡¦ç†
        return df
    except Exception as e:
        raise CsvDownloadError(f"Failed: {e}")
    finally:
        # å¸¸ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if self.session:
            self.session.close()
            self.session = None
```

**ãƒ¡ãƒªãƒƒãƒˆ**: ã‚·ãƒ³ãƒ—ãƒ«
**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**: æ¯å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œã‚Šç›´ã™ï¼ˆéåŠ¹ç‡ï¼‰

### æ¨å¥¨: **ä»•æ§˜æ¡ˆAï¼ˆContext Managerï¼‰**
- Pythonã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ãŒç¢ºå®Ÿ
- å†åˆ©ç”¨å¯èƒ½

---

## 4. test_partial_month_failure

### ç¾çŠ¶
- è¤‡æ•°æœˆã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã§ä¸€éƒ¨ãŒå¤±æ•—ã—ãŸå ´åˆã®æŒ™å‹•ãŒæœªå®šç¾©
- éƒ¨åˆ†æˆåŠŸã‚’è¨±å®¹ã™ã¹ãã‹ã€å…¨å¤±æ•—ã™ã¹ãã‹ä¸æ˜

### ä»•æ§˜æ¡ˆA: Fail Fastï¼ˆå…¨å¤±æ•—ï¼‰

**æ–¹é‡**: 1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰å…¨ä½“ã‚’å¤±æ•—ã¨ã™ã‚‹

```python
async def get_by_location_and_date_range(...) -> WeatherDataWithLocationDTO:
    all_weather_data = []
    
    for month_period in month_periods:
        try:
            month_data = await self._fetch_month_data(...)
            all_weather_data.extend(month_data)
        except HtmlFetchError as e:
            # 1ã¤ã§ã‚‚å¤±æ•—ã—ãŸã‚‰å…¨ä½“ã‚’å¤±æ•—
            raise WeatherAPIError(
                f"Failed to fetch data for {year}-{month}: {e}. "
                f"Aborting entire request for data consistency."
            )
    
    return WeatherDataWithLocationDTO(...)
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãŒä¿è¨¼ã•ã‚Œã‚‹
- æ¬ æãŒãªã„å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—ã§ããªã„
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ãŒæ‚ªã„

### ä»•æ§˜æ¡ˆB: Partial Successï¼ˆéƒ¨åˆ†æˆåŠŸï¼‰

**æ–¹é‡**: å–å¾—ã§ããŸãƒ‡ãƒ¼ã‚¿ã ã‘è¿”ã™

```python
async def get_by_location_and_date_range(...) -> WeatherDataWithLocationDTO:
    all_weather_data = []
    failed_months = []
    
    for month_period in month_periods:
        try:
            month_data = await self._fetch_month_data(...)
            all_weather_data.extend(month_data)
        except HtmlFetchError as e:
            # å¤±æ•—ã‚’è¨˜éŒ²ã™ã‚‹ãŒç¶šè¡Œ
            failed_months.append((year, month))
            self.logger.warning(
                f"Failed to fetch {year}-{month}: {e}. "
                f"Continuing with available data."
            )
    
    # 1ã¤ã‚‚ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
    if not all_weather_data:
        raise WeatherDataNotFoundError(
            f"No weather data available. Failed months: {failed_months}"
        )
    
    # éƒ¨åˆ†çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆè­¦å‘Šä»˜ãï¼‰
    if failed_months:
        self.logger.warning(
            f"Partial data returned. Missing data for: {failed_months}"
        )
    
    return WeatherDataWithLocationDTO(...)
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã‚‹
- éƒ¨åˆ†çš„ãªéšœå®³ã«å¼·ã„

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ï¼ˆæ¬ æãŒã‚ã‚‹ï¼‰
- ä¸‹æµå‡¦ç†ã§æ¬ æã‚’è€ƒæ…®ã™ã‚‹å¿…è¦

### ä»•æ§˜æ¡ˆC: Retry with Fallback

**æ–¹é‡**: å¤±æ•—ã—ãŸã‚‰å†è©¦è¡Œã€ãã‚Œã§ã‚‚å¤±æ•—ãªã‚‰éƒ¨åˆ†æˆåŠŸ

```python
async def get_by_location_and_date_range(...) -> WeatherDataWithLocationDTO:
    all_weather_data = []
    
    for month_period in month_periods:
        month_data = await self._fetch_month_data_with_retry(
            year, month, prec_no, block_no, max_retries=3
        )
        
        if month_data:
            all_weather_data.extend(month_data)
        # month_dataãŒNoneã§ã‚‚ç¶šè¡Œï¼ˆè­¦å‘Šã¯å‡ºã™ï¼‰
    
    if not all_weather_data:
        raise WeatherDataNotFoundError(...)
    
    return WeatherDataWithLocationDTO(...)

async def _fetch_month_data_with_retry(
    self, year: int, month: int, prec_no: int, block_no: int,
    max_retries: int = 3
) -> Optional[List[WeatherData]]:
    """Fetch month data with retry logic."""
    for attempt in range(max_retries):
        try:
            return await self._fetch_month_data(year, month, prec_no, block_no)
        except HtmlFetchError as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                self.logger.error(f"Failed after {max_retries} attempts: {e}")
                return None  # Give up
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- ä¸€æ™‚çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ã«å¼·ã„
- ãƒ‡ãƒ¼ã‚¿å–å¾—ç‡ãŒå‘ä¸Š

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- è¤‡é›‘
- å®Ÿè¡Œæ™‚é–“ãŒé•·ããªã‚‹å¯èƒ½æ€§

### æ¨å¥¨: **ä»•æ§˜æ¡ˆBï¼ˆPartial Successï¼‰**
- å®Ÿç”¨æ€§é‡è¦–
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å–å¾—ã§ãã‚‹
- ãƒ­ã‚°ã§æ¬ æã‚’æ˜ç¤º
- å°†æ¥çš„ã«ä»•æ§˜æ¡ˆCã‚’è¿½åŠ æ¤œè¨

---

## 5. test_duplicate_dates_in_csvï¼ˆè©³ç´°ä»•æ§˜ï¼‰

### å®Ÿè£…æ¡ˆ

```python
def _parse_jma_table(
    self,
    table: HtmlTable,
    start_date: str,
    end_date: str,
    year: int,
    month: int
) -> List[WeatherData]:
    """Parse JMA HTML table with duplicate detection."""
    weather_data_dict: Dict[datetime, WeatherData] = {}
    duplicate_count = 0
    
    for row in table.rows:
        try:
            # ... ãƒ‡ãƒ¼ã‚¿è§£æ ...
            
            record_date = datetime(year, month, day)
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if record_date in weather_data_dict:
                duplicate_count += 1
                self.logger.warning(
                    f"Duplicate date detected: {record_date.date()}. "
                    f"Keeping first occurrence, discarding duplicate."
                )
                continue  # First Wins
            
            # Filter by date range
            if record_date < start_dt or record_date > end_dt:
                continue
            
            weather_data = WeatherData(...)
            
            # Validate before storing
            date_str = f"{year}-{month:02d}-{day:02d}"
            if self._validate_weather_data(weather_data, date_str):
                weather_data_dict[record_date] = weather_data
            
        except Exception as e:
            self.logger.warning(f"Failed to parse row: {e}")
            continue
    
    # é‡è¤‡ãŒã‚ã£ãŸå ´åˆã¯è­¦å‘Š
    if duplicate_count > 0:
        self.logger.warning(
            f"Detected {duplicate_count} duplicate dates in data. "
            f"Data quality issue."
        )
    
    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    return sorted(weather_data_dict.values(), key=lambda x: x.time)
```

**é‡è¤‡æ¤œå‡ºã®è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
```python
class WeatherDataQualityMetrics:
    """Data quality metrics for monitoring."""
    duplicate_dates: int = 0
    missing_columns: int = 0
    invalid_values: int = 0
    total_rows: int = 0
    
    @property
    def quality_score(self) -> float:
        """Calculate data quality score (0-1)."""
        if self.total_rows == 0:
            return 1.0
        issues = self.duplicate_dates + self.missing_columns + self.invalid_values
        return max(0.0, 1.0 - (issues / self.total_rows))
```

---

## 6. test_session_cleanup_on_errorï¼ˆè©³ç´°å®Ÿè£…ï¼‰

### å®Œå…¨ãªå®Ÿè£…æ¡ˆ

```python
# src/agrr_core/framework/repositories/csv_downloader.py

import requests
import pandas as pd
from io import StringIO
from typing import Optional
from contextlib import asynccontextmanager
import asyncio

from agrr_core.entity.exceptions.csv_download_error import CsvDownloadError
from agrr_core.adapter.interfaces.csv_service_interface import CsvServiceInterface


class CsvDownloader(CsvServiceInterface):
    """CSV downloader with proper resource management."""
    
    def __init__(self, timeout: int = 30):
        """Initialize CSV downloader."""
        self.timeout = timeout
        self._session: Optional[requests.Session] = None
        self._is_closed = False
    
    def _ensure_session(self):
        """Ensure session exists."""
        if self._session is None or self._is_closed:
            self._session = requests.Session()
            self._is_closed = False
    
    async def download_csv(
        self,
        url: str,
        encoding: str = 'utf-8'
    ) -> pd.DataFrame:
        """Download and parse CSV data with automatic cleanup on error."""
        self._ensure_session()
        
        try:
            # Run blocking I/O in thread pool
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self._session.get(url, timeout=self.timeout)
            )
            response.raise_for_status()
            
            # Parse CSV
            csv_text = response.content.decode(encoding)
            df = pd.read_csv(StringIO(csv_text))
            
            return df
            
        except requests.RequestException as e:
            # Clean up on error
            self.close()
            raise CsvDownloadError(f"Failed to download CSV from {url}: {e}")
        except Exception as e:
            # Clean up on any error
            self.close()
            raise CsvDownloadError(f"Failed to process CSV: {e}")
    
    def close(self):
        """Close session and cleanup resources."""
        if self._session and not self._is_closed:
            self._session.close()
            self._is_closed = True
            self._session = None
    
    def __del__(self):
        """Destructor to ensure cleanup."""
        self.close()
    
    def __enter__(self):
        """Context manager entry."""
        self._ensure_session()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()
        return False
    
    async def __aenter__(self):
        """Async context manager entry."""
        self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        self.close()
        return False
```

**ä½¿ç”¨ä¾‹**:
```python
# åŒæœŸçš„ãªä½¿ç”¨
with CsvDownloader(timeout=30) as downloader:
    df = await downloader.download_csv(url)

# éåŒæœŸçš„ãªä½¿ç”¨
async with CsvDownloader(timeout=30) as downloader:
    df = await downloader.download_csv(url)
```

**ãƒ†ã‚¹ãƒˆã®æ›´æ–°**:
```python
@pytest.mark.asyncio
async def test_session_cleanup_on_error():
    """Test that CSV downloader cleans up resources on error."""
    downloader = CsvDownloader(timeout=1)
    
    try:
        await downloader.download_csv("http://invalid-url.test")
    except CsvDownloadError:
        pass  # Expected
    
    # Session should be cleaned up
    assert downloader._is_closed is True
    assert downloader._session is None
```

---

## 7. XPASSå¯¾å¿œï¼ˆå®Ÿè£…æ¸ˆã¿ãƒ†ã‚¹ãƒˆï¼‰

### test_all_null_temperature_values

**å¯¾å¿œ**: xfailãƒãƒ¼ã‚¯ã‚’å‰Šé™¤

```python
# tests/test_adapter/test_weather_jma_repository_critical.py

# Before:
@pytest.mark.xfail(reason="Data quality validation not implemented yet")
@pytest.mark.asyncio
async def test_all_null_temperature_values(...):

# After:
@pytest.mark.asyncio  # xfailå‰Šé™¤
async def test_all_null_temperature_values(...):
    """Test handling of HTML table with all null temperature values."""
    # ã“ã®ãƒ†ã‚¹ãƒˆã¯æˆåŠŸã™ã‚‹ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰
```

### test_distance_calculation_hokkaido_okinawa

**å¯¾å¿œæ¡ˆ1**: xfailãƒãƒ¼ã‚¯ã‚’å‰Šé™¤ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã§ååˆ†ï¼‰
```python
# xfailå‰Šé™¤ - ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã§å®Ÿç”¨ä¸Šå•é¡Œãªã—
def test_distance_calculation_hokkaido_okinawa(...):
```

**å¯¾å¿œæ¡ˆ2**: Haversineè·é›¢ã‚’å®Ÿè£…
```python
import math

def _haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculate Haversine distance in degrees (approximation)."""
    R = 6371  # Earth radius in km
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    
    a = (math.sin(dlat / 2) ** 2 +
         math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *
         math.sin(dlon / 2) ** 2)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    
    return R * c  # Distance in km
```

**æ¨å¥¨**: å¯¾å¿œæ¡ˆ1ï¼ˆxfailå‰Šé™¤ï¼‰- æ—¥æœ¬å›½å†…ã§ã¯ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã§ååˆ†

### test_year_boundary_crossing

**å¯¾å¿œ**: xfailãƒãƒ¼ã‚¯ã‚’å‰Šé™¤ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰

```python
# xfailå‰Šé™¤ - relativedeltaã§æ­£ã—ãå‡¦ç†ã•ã‚Œã¦ã„ã‚‹
@pytest.mark.asyncio
async def test_year_boundary_crossing(...):
    """Test date range crossing year boundary."""
```

---

## å®Ÿè£…å„ªå…ˆé †ä½

### Phase 1ï¼ˆå³æ™‚å¯¾å¿œï¼‰: XPASSè§£æ¶ˆ
1. âœ… `test_all_null_temperature_values` - xfailå‰Šé™¤
2. âœ… `test_year_boundary_crossing` - xfailå‰Šé™¤
3. âœ… `test_distance_calculation_hokkaido_okinawa` - xfailå‰Šé™¤ã¾ãŸã¯reasonæ›´æ–°

### Phase 2ï¼ˆçŸ­æœŸ: 1-2é€±é–“ï¼‰: é«˜å„ªå…ˆåº¦
1. ğŸ”´ `test_missing_required_columns` - Strict ãƒ¢ãƒ¼ãƒ‰å®Ÿè£…
2. ğŸŸ¡ `test_duplicate_dates_in_csv` - First Wins + è­¦å‘Šå®Ÿè£…

### Phase 3ï¼ˆä¸­æœŸ: 1-2ãƒ¶æœˆï¼‰: ä¸­å„ªå…ˆåº¦
1. ğŸŸ¡ `test_session_cleanup_on_error` - Context Managerå®Ÿè£…

### Phase 4ï¼ˆé•·æœŸ: ä»•æ§˜ç¢ºå®šå¾Œï¼‰: ä½å„ªå…ˆåº¦
1. ğŸŸ¢ `test_partial_month_failure` - ä»•æ§˜æ±ºå®šå¾Œã«å®Ÿè£…

---

## ã¾ã¨ã‚

### xfailä½¿ç”¨ã®æ­£å½“æ€§
- âœ… 4ã¤ã®XFAILã¯å…¨ã¦æ­£å½“ï¼ˆæœªå®Ÿè£…æ©Ÿèƒ½ã‚’æ–‡æ›¸åŒ–ï¼‰
- âš ï¸ 3ã¤ã®XPASSã¯æŠ€è¡“çš„è² å‚µï¼ˆå®Ÿè£…æ¸ˆã¿ãªã®ã«ãƒãƒ¼ã‚¯ãŒæ®‹ã£ã¦ã„ã‚‹ï¼‰

### æ¨å¥¨ä»•æ§˜
1. **Missing Columns**: Strict ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªå„ªå…ˆï¼‰
2. **Duplicate Dates**: First Wins + è­¦å‘Šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§äºˆæ¸¬å¯èƒ½ï¼‰
3. **Session Cleanup**: Context Managerï¼ˆPythonicï¼‰
4. **Partial Failure**: Partial Successï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“å„ªå…ˆï¼‰

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. XPASSã®xfailãƒãƒ¼ã‚¯ã‚’å‰Šé™¤ï¼ˆå³æ™‚ï¼‰
2. å„ªå…ˆåº¦ã®é«˜ã„XFAILã‹ã‚‰å®Ÿè£…é–‹å§‹

